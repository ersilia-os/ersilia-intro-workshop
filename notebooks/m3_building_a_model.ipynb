{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlj3oIUnGb6UzA5+tpFkBX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ersilia-os/ersilia-intro-workshop/blob/main/notebooks/m3_building_a_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train an AI model\n",
        "\n",
        "This notebook contains the basic steps to train a classifier for bioactivity prediction.\n",
        "\n",
        "It is prepared to run on Google Colaboratory, if you want to run it locally make sure to create a conda environment with Python 3.10 and install the packages indicated below.\n",
        "\n",
        "_Remember that the ! sign indicates a bash command, to run it in the terminal simply copy the command without !_"
      ],
      "metadata": {
        "id": "KNF8_p8BP9hZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Machine Learning\n",
        "\n",
        "Uses previously **labeled** data to train an algorithm (i.e the output is known). The algorithm learns if it is doing right by comparing the predicted vs the real output.\n",
        "\n",
        "Simplified steps:\n",
        "1. Data collection and processing.\n",
        "2. Division of training data in Train and Test sets.\n",
        "3. Use the train set to train the model.\n",
        "4. Predict an output for the test set and compare the predicted vs real results.\n",
        "5. Improve the model until we are satisfied with the performance on the test set\n",
        "\n",
        "## Types of supervised ML models\n",
        "### Classification\n",
        "Classification problems are characterized by having categorical output (i.e: active, inactive), so the model tries to predict to which class the input belongs. It can include several classes, is not limited to a binary classification.\n",
        "\n",
        "### Regression\n",
        "Regression problems are characterized by continuous variables, where the model tries to predict the exact value of the input (i.e: IC50 of a specific compound)\n",
        "\n",
        "## Evaluation of supervised ML models\n",
        "\n",
        "### Classification metrics\n",
        "\n",
        "We obtain the following data to evaluate the model:\n",
        "- Y_pred: predictions on the test set\n",
        "- Y_real: real outcome of the test set\n",
        "\n",
        "**Accuracy:** number of correct predictions divided by the total number of predictions (TP/(lenY)). For example, if we have predicted correctly 5 out of 10 data points --> Accuracy = 50%\n",
        "\n",
        "**Precision**: identification of only real positives (with a 100% precision, a model does not classify any negatives as positive) --> TP/(TP+FP)\n",
        "\n",
        "**Recall**: identification of all positives (with a 100% recall, no positive is classified as negative, but some negatives might be included in the positives) --> TP / (TP+FN)\n",
        "\n",
        "**Confusion matrix**: plots the real vs the predicted values in a table, to easily obtain the FP, TP, TN, FN values.\n",
        "\n",
        "\n",
        "### Regression metrics\n",
        "In a regression task, we obtain as error the difference between the predicted value and the real value (i.e: IC50real=0.1, IC50pred = 0.5 --> error of 0.4).\n",
        "\n",
        "\n",
        "**Mean Absolute Error:** mean of the absolute values of errors.\n",
        "\n",
        "**Mean Squared Error:** mean of the square error. By squaring, larger errors are contributing more and therefore the model punishes them.\n",
        "\n",
        "**Root Mean Squared Error:** root of the mean of square error to simplify interpretation (by using MSE, we also square the units which makes them difficult to interpret).\n",
        "\n",
        "**R-square**: coefficient of determination, the amount of variance explained by the model (from 0 to 1, the closer to 1, the better our model is)"
      ],
      "metadata": {
        "id": "xsEoeu3uuV33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install the necessary packages\n",
        "\n",
        "For this exercise, we will need the following packages:\n",
        "- RdKit: provides basic chemoinformatics tools\n",
        "- Pandas: deals with tabular data (like csv files)\n",
        "- Matplotlib and seaborn: plotting tools to visualise results\n",
        "- LazyQSAR: a simple package to build fast AI models developed by Ersilia. LazyQSAR automatically installs RdKit, Scikit-Learn, Ersilia Compound Embeddings, FLAML, Mordred and Numpy among others\n",
        "\n",
        "Google Colab already provides Pandas, Matplotlib and Seaborn pre-installed, so you do not need to install them. If you run this notebook locally please make sure to use the `pip install` command on the Terminal to get the packages in your conda environment\n",
        "\n",
        "**If you restart the notebook, you will need to reinstall the packages. In a local environment, you do NOT need to reinstall the packages, just activate the conda environment again**"
      ],
      "metadata": {
        "id": "IRzpkaSJQ5qy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiqqsPt0Eduo"
      },
      "outputs": [],
      "source": [
        "# Package installation\n",
        "!pip install lazyqsar==0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load your data\n",
        "\n",
        "Next, we will use the pandas package to load the training data. At the bare minimmum, you should have a .csv file with two columns: SMILES and Activity of the molecule against the desired target.\n",
        "\n",
        "In Google Colab, go to the left hand side of the screen and click on the `folder` icon. There, you will see a `Upload file` icon to add your files to the Cloud. You can also call them from Google Drive. These files will be deleted if the notebook is closed, so you will need to re-upload them every time.\n",
        "\n",
        "If you are working locally, just make sure to input the right path to your data file."
      ],
      "metadata": {
        "id": "-goEzL25T-p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title File Name\n",
        "filename = \"chembl325_processed.csv\" # @param {type:\"string\"}\n"
      ],
      "metadata": {
        "id": "apvdFbJkD8Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data into a Pandas DataFrame (df)\n",
        "# We will use the read_csv function from Pandas to automatically upload the csv file in the right format for Python\n",
        "# We need first to import the package Pandas, which we abbreviate as pd\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(filename)"
      ],
      "metadata": {
        "id": "hmZhASb9Rk1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can now observe how our dataframe looks like\n",
        "df"
      ],
      "metadata": {
        "id": "CVtJcEc0VE8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and its shape\n",
        "df.shape"
      ],
      "metadata": {
        "id": "mvBsjtHtVIQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get the variables right!\n",
        "# If the column name changes, you need to change these variables\n",
        "# Remember Python strings are case-sensitive\n",
        "SMILES = \"SMILES\"\n",
        "EXP = \"pIC50\"\n",
        "BIN = \"bin\""
      ],
      "metadata": {
        "id": "DhNGnvO5559K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Define a cut-off\n",
        "\n",
        "Since we will create a classifier, we need to decide at which cut-off we consider our molecules `Active` or `Inactive`.\n",
        "\n",
        "We can visualise our data with a histogram for example to make the decision, or we can ask the original data producers for their expert recommendation."
      ],
      "metadata": {
        "id": "zSEpGWggVPp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We import the plotting package Matplotlib, abbreviated as plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# We select the variable we want to plot in the histogram, in this case the activity values\n",
        "x = df[EXP] #here you need to write the exact column name, respecting Caps\n",
        "print(x)"
      ],
      "metadata": {
        "id": "8xmKsGu5VLv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We now plot the data using the histogram function\n",
        "plt.hist(x)"
      ],
      "metadata": {
        "id": "SvjgNIsvVz0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Depending on our data, we might want to improve the plot for better visualization\n",
        "# here, we clip the maximmum values to 20\n",
        "plt.hist(x, range=(x.min(), 20))\n",
        "plt.xlabel(\"IC50 in uM\")\n",
        "plt.ylabel(\"Number of mols\")"
      ],
      "metadata": {
        "id": "gMphsUGjWDmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To identify the best activity cut-off for your data, you need to:\n",
        "- Understand the experimental set up: what kind of assay was done? what is the units of activity measured?\n",
        "- Understand what is the end goal for the model: is it obtaining the most active compounds? Is it to filter our some non-actives to reduce the test number?\n",
        "- Ask the experts! Talk to the people who ran the experiment, if possible\n",
        "\n",
        "Once you have the cut-off, transform the activities to binary. Typically, by consensus:\n",
        "- 1: Active (the compound perturbs our biological system)\n",
        "- 0: Inactive (the compound does not affect our biolgical system)"
      ],
      "metadata": {
        "id": "w6XkTh6Qhh81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Activity cut-off { display-mode: \"form\" }\n",
        "# We identify a good cut-off for our data and assign the variable cutoff\n",
        "cutoff = 7 # @param {type:\"number\"}"
      ],
      "metadata": {
        "id": "Cd_dkIydi0ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Direction of activity { display-mode: \"form\" }\n",
        "# Are our actives lower than the cut-off, or higher than the cut-off?\n",
        "direction = \"higher\" # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "TukAkeBRjRIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here, we will create a new column in our file with the binary activity\n",
        "\n",
        "if direction == \"lower\":\n",
        "    df[BIN] = [1 if x <= cutoff else 0 for x in df[EXP]]\n",
        "elif direction == \"higher\":\n",
        "    df[BIN] = [1 if x >= cutoff else 0 for x in df[EXP]]\n",
        "else:\n",
        "    print(\"no direction specified. Please select lower or higher direction for the activity cut-off\")"
      ],
      "metadata": {
        "id": "mf-bD2Y3YYnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the output\n",
        "df"
      ],
      "metadata": {
        "id": "Tz5SbNKUihjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can print how many actives / inactives we have in our dataset\n",
        "print(\"Total molecules: \", len(df))\n",
        "print(\"Active molecules: \", len(df[df[BIN]==1]))\n",
        "print(\"Inactive molecules: \", len(df[df[BIN]==0]))\n",
        "print(\"Frequency of Actives (%): \", len(df[df[BIN]==1])/len(df)*100)"
      ],
      "metadata": {
        "id": "d6YSdNnRkd5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Divide the data into Train and Test sets\n",
        "\n",
        "To make sure our model will have good performance, we need to keep part of the data as a test set, to evaluate the model once trained.\n",
        "\n",
        "Typically, it is good practice to reserve ~20% of the data as test set. Make sure that the balance of actives / inactives is maintained both in the train and the test sets"
      ],
      "metadata": {
        "id": "FFM0ERcUj3uL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use this function to split the data\n",
        "# The function uses the sklearn train_test_split built-in method\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "def random_split(df, size):\n",
        "    indices = np.arange(len(df))\n",
        "    X_train, X_test, y_train, y_test, i_train, i_test = train_test_split(df[SMILES], df[EXP], indices, test_size=size, stratify=df[BIN])\n",
        "    train = df.iloc[i_train]\n",
        "    test = df.iloc[i_test]\n",
        "    return train, test"
      ],
      "metadata": {
        "id": "znw48FdFj0Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = random_split(df, 0.2)"
      ],
      "metadata": {
        "id": "WAC8hkvLlX8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the train set\n",
        "train"
      ],
      "metadata": {
        "id": "4aPTXTeZlcom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the test set\n",
        "test"
      ],
      "metadata": {
        "id": "SL9w9_lzldd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can make sure the active / inactive balance was preserved\n",
        "train_balance = len(train[train[BIN]==1])/len(train)*100\n",
        "test_balance = len(test[test[BIN]==1])/len(test)*100\n",
        "\n",
        "print(\"The frequency of Actives in the train set is: \", train_balance)\n",
        "print(\"The frequency of Actives in the test set is: \", test_balance)"
      ],
      "metadata": {
        "id": "zFotFHxnleQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also save the train and test sets in case we want to explore them manually\n",
        "# To save files, we use the pandas method \"to_csv\". Make sure to write the right path to your file if you are working locally\n",
        "\n",
        "train.to_csv(\"train_set.csv\", index=False)\n",
        "test.to_csv(\"test_set.csv\", index=False)"
      ],
      "metadata": {
        "id": "GLDNwO2ll9Fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Featurization of molecules\n",
        "\n",
        "The next step is to convert the molecules in our train set to vectors so that we can pass them as input for the machine learning model.\n",
        "We will first use the most used fingerprint, the Morgan Descriptors, which can be easily calculated using RdKit."
      ],
      "metadata": {
        "id": "FPuAcgm1mnn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "\n",
        "# Create the molecules from the SMILES string\n",
        "smiles = train[SMILES]  # Replace this with your own SMILES string\n",
        "mols = [Chem.MolFromSmiles(smi) for smi in smiles]\n",
        "\n",
        "# Generate Morgan fingerprints\n",
        "radius = 3  # Specify the radius of the circular fingerprint\n",
        "nBits = 2048\n",
        "fps = [AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits) for mol in mols]\n",
        "morgan_fps_train = [np.array(list(fp.ToBitString())).astype(int) for fp in fps]"
      ],
      "metadata": {
        "id": "tM0zkJuYmWTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's check the outcome!\n",
        "morgan_fps_train"
      ],
      "metadata": {
        "id": "e26xbwVFp-El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_* We have individually created the descriptors so that we can check how they look like. Luckily, LazyQSAR provides a single funtion to convert the smiles to vectors and train an ML model, so we will use that feature_"
      ],
      "metadata": {
        "id": "d-1cJPtbqZdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Train an ML model\n",
        "\n",
        "We have our input (morgan descriptors) and the endpoint, the activity in binary format (1 or 0) so we are ready to train a classifier!\n",
        "We will use the built-in method in LazyQSAR"
      ],
      "metadata": {
        "id": "BXb5IfUeqMNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lazyqsar as lq\n",
        "\n",
        "smiles_train = train[SMILES]\n",
        "y_train = train[BIN]\n",
        "model = lq.MorganBinaryClassifier(time_budget_sec=60, estimator_list=[\"rf\", \"lgbm\", \"xgboost\"])\n",
        "model.fit(smiles_train, y_train)"
      ],
      "metadata": {
        "id": "yo3CxlLhqADe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Evaluate the model performance\n",
        "\n",
        "After fitting the model, we need to evaluate how good it works on the test set. For that, we will run the model to make predictions with the test data and compare it to the real values."
      ],
      "metadata": {
        "id": "fwox_bgYsFQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We predict the probabilities of the model for the smiles in the test set\n",
        "# We can pass SMILES directly because our method converts them to Morgan fingerprints\n",
        "\n",
        "smiles_test = test[SMILES]\n",
        "y_hat = model.predict_proba(smiles_test)\n",
        "y_hat"
      ],
      "metadata": {
        "id": "SpjR8Pi-r0pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We get two results for each molecule, why?\n",
        "# We only want to keep the probability of Active (1), the second column\n",
        "y_hat = y_hat[:,1]\n",
        "y_hat"
      ],
      "metadata": {
        "id": "MixWPoI9swNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The ROC-AUC curve"
      ],
      "metadata": {
        "id": "XUILW9kTsXBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# We need the real results, the activity of the test set\n",
        "y_test = test[BIN]\n",
        "\n",
        "# We use the sklearn package to calculate the roc_curve and plot it\n",
        "fpr, tpr, _ = roc_curve(y_test, y_hat)\n",
        "auroc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='#50285a', lw=2, label=f'ROC curve (AUC = {auroc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='#bee6b4', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ARqY5XHKsWOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Contingency table\n",
        "\n",
        "The contingency table tells us how many molecules will be correctly predicted and how many will be false positives or false negatives.\n",
        "\n",
        "For that, we need to convert the continuous model output (probabilities) to a binary outcome (1 or 0) using the cut-off we decide between 0 and 1. Typically, the cut-off is assigned at 0.5."
      ],
      "metadata": {
        "id": "sUawwObXsY9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "proba_cutoff = 0.5\n",
        "y_hat_bin = [1 if x >= proba_cutoff else 0 for x in y_hat]\n",
        "\n",
        "\n",
        "cf_matrix = confusion_matrix(y_test, y_hat_bin)\n",
        "\n",
        "ax = sns.heatmap(cf_matrix, annot=True, cmap='Greens', cbar=False, annot_kws={\"size\": 16})\n",
        "ax.set_title(\"Predicted vs Real\", fontsize=14)\n",
        "ax.set_xlabel('Predicted', fontsize=14)\n",
        "ax.set_ylabel('Real', fontsize=14)\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels(['Inactive','Active'], fontsize=14)\n",
        "ax.yaxis.set_ticklabels(['Inactive','Active'], fontsize=14)"
      ],
      "metadata": {
        "id": "sjnFzzoxsbVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Classification Report\n",
        "\n",
        "Sklearn can automatically provide a report with the measures of precision and recall for each class"
      ],
      "metadata": {
        "id": "Q7847lM2uhWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test,y_hat_bin))"
      ],
      "metadata": {
        "id": "p0iL6Lw8ujT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Save the model\n",
        "If we want to reuse our model, we need to save it so it can be applied to new data!"
      ],
      "metadata": {
        "id": "b6vUU6KFv5g-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"model_morgan.joblib\")"
      ],
      "metadata": {
        "id": "Cp9U_-5Mv4aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. What could we do to improve model performace?\n",
        "- Use more time for training\n",
        "- Try another featurizer method\n",
        "- ...\n",
        "\n",
        "In this example, we will test and evaluate another method for featurizing, the Ersilia Compound Embeddings."
      ],
      "metadata": {
        "id": "Ynip8QoLwjhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a model using LazyQSAR and evaluate its performance\n",
        "\n",
        "import lazyqsar as lq\n",
        "\n",
        "smiles_train = train[SMILES]\n",
        "y_train = train[BIN]\n",
        "\n",
        "model = lq.ErsiliaBinaryClassifier(time_budget_sec=60, estimator_list=[\"rf\", \"lgbm\", \"xgboost\"])\n",
        "model.fit(smiles_train, y_train)"
      ],
      "metadata": {
        "id": "XBD_BTjMuoo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat"
      ],
      "metadata": {
        "id": "M09d8CbjBzXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smiles_test = test[SMILES]\n",
        "\n",
        "y_hat = model.predict_proba(smiles_test)[:,1]\n",
        "# We need the real results, the activity of the test set\n",
        "y_test = test[BIN]\n",
        "\n",
        "# We use the sklearn package to calculate the roc_curve and plot it\n",
        "fpr, tpr, _ = roc_curve(y_test, y_hat)\n",
        "auroc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='#50285a', lw=2, label=f'ROC curve (AUC = {auroc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='#bee6b4', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "K7hR9cbfw9Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proba_cutoff = 0.5\n",
        "y_hat_bin = [1 if x >= proba_cutoff else 0 for x in y_hat]\n",
        "\n",
        "cf_matrix = confusion_matrix(y_test, y_hat_bin)\n",
        "\n",
        "ax = sns.heatmap(cf_matrix, annot=True, cmap='Greens', cbar=False, annot_kws={\"size\": 16})\n",
        "ax.set_title(\"Predicted vs Real\", fontsize=14)\n",
        "ax.set_xlabel('Predicted', fontsize=14)\n",
        "ax.set_ylabel('Real', fontsize=14)\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels(['Inactive','Active'], fontsize=14)\n",
        "ax.yaxis.set_ticklabels(['Inactive','Active'], fontsize=14)"
      ],
      "metadata": {
        "id": "Vv9P-TPNxKZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test,y_hat_bin))"
      ],
      "metadata": {
        "id": "3QlQOY_qxN8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"model_eosce.joblib\")"
      ],
      "metadata": {
        "id": "4E7F_UUTxUBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Save the final model\n",
        "\n",
        "To ensure we use all possible data, at the end of the pipeline we will train the model with the whole dataset (train and test) and save it as the final model for deployment.\n",
        "\n",
        "We need to choose which descriptor and parameters will work better for our case and build the model accordingly. We obtain that from the best train-test experiment."
      ],
      "metadata": {
        "id": "Q7WSSnM0q7Jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model with the full data\n",
        "\n",
        "# load the whole dataset\n",
        "\n",
        "df = pd.read_csv(filename)\n",
        "\n",
        "# binarize the activity if needed\n",
        "SMILES = \"SMILES\"\n",
        "EXP = \"pIC50\"\n",
        "BIN = \"bin\"\n",
        "\n",
        "direction = \"lower\"\n",
        "cutoff = 2.5\n",
        "\n",
        "if direction == \"lower\":\n",
        "    df[BIN] = [1 if x <= cutoff else 0 for x in df[EXP]]\n",
        "elif direction == \"higher\":\n",
        "    df[BIN] = [1 if x >= cutoff else 0 for x in df[EXP]]\n",
        "else:\n",
        "    print(\"no direction specified. Please select lower or higher direction for the activity cut-off\")\n",
        "\n",
        "# train the selected model\n",
        "import lazyqsar as lq\n",
        "smiles_train = df[SMILES]\n",
        "y_train = df[BIN]\n",
        "model = lq.MorganBinaryClassifier(time_budget_sec=60, estimator_list=[\"rf\", \"lgbm\", \"xgboost\"])\n",
        "model.fit(smiles_train, y_train)\n",
        "model.save(\"final_model.joblib\")"
      ],
      "metadata": {
        "id": "TbdUVEkdq4_1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}